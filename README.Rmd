---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# svrep

<!-- badges: start -->
[![R-CMD-check](https://github.com/bschneidr/svrep/workflows/R-CMD-check/badge.svg)](https://github.com/bschneidr/svrep/actions)
[![Codecov test coverage](https://codecov.io/gh/bschneidr/svrep/branch/main/graph/badge.svg)](https://app.codecov.io/gh/bschneidr/svrep?branch=main)
<!-- badges: end -->

svrep provides methods for creating, updating, and analyzing replicate weights for surveys. Functions from svrep can be used to implement adjustments to replicate designs (e.g. nonresponse weighting class adjustments) and analyze their effect on the replicate weights and on estimates of interest.

## Installation

You can install the released version of svrep from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("svrep")
```

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("bschneidr/svrep")
```

## Example usage

Suppose we have a replicate-weights survey design object created with the survey package. This survey design object can include respondents, non-respondents, and cases with unknown eligibility.

```{r, message=FALSE, warning=FALSE}
library(survey)
library(svrep)
data(api, package = "survey")
set.seed(2021)

# Create variable giving response status
apiclus1$response_status <- sample(x = c("Respondent", "Nonrespondent",
                                         "Ineligible", "Unknown eligibility"),
                                   size = nrow(apiclus1),
                                   replace = TRUE)

# Create replicate-weights survey design
dclus1 <- svydesign(data = apiclus1,
                    id = ~dnum, weights = ~pw, fpc = ~fpc)

orig_rep_design <- as.svrepdesign(dclus1)

print(orig_rep_design)
```

### Adjusting for non-response or unknown eligibility

It is common practice to adjust weights when there is non-response or there are sampled cases whose eligibility for the survey is unknown. The most common form of adjustment is "weight redistribution": for example, weights from non-respondents are reduced to zero, and weights from respondents are correspondingly increased so that the total weight in the sample is unchanged. In order to account for these adjustments when estimating variances for survey statistics, the adjustments are repeated separately for each set of replicate weights. This process can be easily implemented using the `redistribute_weights()` function.

```{r}
# Adjust weights for unknown eligibility
ue_adjusted_design <- redistribute_weights(design = orig_rep_design,
                                           reduce_if = response_status %in% c("Unknown eligibility"),
                                           increase_if = !response_status %in% c("Unknown eligibility"))
```

By supplying column names to the `by` argument of `redistribute_weights()`, adjustments are conducted separately in different groups. This can be used to conduct nonresponse weighting class adjustments.

```{r}
nr_adjusted_design <- redistribute_weights(design = ue_adjusted_design,
                                           reduce_if = response_status == "Nonrespondent",
                                           increase_if = response_status == "Respondent",
                                           by = c("stype"))
```

### Comparing estimates from different sets of weights

In order to assess whether weighting adjustments have an impact on the estimates we care about, we want to compare the estimates from the different sets of weights. The function `svyby_repwts()` makes it easy to compare estimates from different sets of weights.

```{r}
# Estimate overall means (and their standard errors) from each design
overall_estimates <- svyby_repwts(
  rep_designs = list('original' = orig_rep_design,
                     'nonresponse-adjusted' = nr_adjusted_design),
  formula = ~ api00, FUN = svymean
)
print(overall_estimates, row.names = FALSE)

# Estimate domain means (and their standard errors) from each design
domain_estimates <- svyby_repwts(
  rep_designs = list('original' = orig_rep_design,
                     'nonresponse-adjusted' = nr_adjusted_design),
  formula = ~ api00, by = ~ stype, FUN = svymean
)
print(domain_estimates, row.names = FALSE)
```

We can even test for differences in estimates from the two sets of weights and calculate confidence intervals for their difference.

```{r}
estimates <- svyby_repwts(
  rep_designs = list('original' = orig_rep_design,
                     'nonresponse-adjusted' = nr_adjusted_design),
  formula = ~ api00, FUN = svymean
)

vcov(estimates)

diff_between_ests <- svycontrast(stat = estimates,
                                 contrasts = list(
                                   "Original vs. Adjusted" = c(-1,1)
                                 ))
print(diff_between_ests)
confint(diff_between_ests)
```

### Diagnosing potential issues with weights

When adjusting replicate weights, there are several diagnostics which can be used to ensure that the adjustments were carried out correctly and that they do more good than harm. The function `summarize_rep_weights()` helps by allowing you to quickly summarize the replicate weights. 

For example, we can check to see that post-stratification correctly removes variation in the column sums of the replicate weights.

```{r}
# Post-stratify the design
post_stratified_design <- postStratify(design = orig_rep_design,
                                       strata = ~ stype + awards,
                                       population = xtabs(~stype + awards,
                                                          data = apipop))
# Compare overall summaries of the weights
list(
  'original' = summarize_rep_weights(orig_rep_design,
                                     type = 'overall'),
  'post-stratified' = summarize_rep_weights(post_stratified_design,
                                            type = 'overall')
)
```

When carrying out nonresponse adjustments, we might want to make sure that column sums are the same before and after the adjustments.

```{r}
# Summarize each column of replicate weights,
# before and after non-response adjustments
orig_rep_col_summaries <- summarize_rep_weights(orig_rep_design, type = 'specific')
adj_rep_col_summaries <- summarize_rep_weights(nr_adjusted_design, type = 'specific')

head(adj_rep_col_summaries)

# Compare the sums and number of nonzero entries
# before and after adjustment
weight_summaries  <- rbind(cbind(orig_rep_col_summaries, Design = 'Original'),
                           cbind(adj_rep_col_summaries, Design = 'NR-adjusted'))

weight_summaries <- weight_summaries[,c("Rep_Column", "Design", "N_NONZERO", "SUM")]
weight_summaries <- weight_summaries[order(weight_summaries$Design, decreasing = TRUE),]
weight_summaries <- weight_summaries[order(weight_summaries$Rep_Column),]
rownames(weight_summaries) <- NULL

head(weight_summaries)
```

